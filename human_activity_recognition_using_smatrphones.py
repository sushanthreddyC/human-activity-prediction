# -*- coding: utf-8 -*-
"""Human activity recognition using smatrphones

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9tdQRtoS_GlahueOY6GqqX8eBpPUGpq
"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

test_orig=pd.read_csv("/content/drive/My Drive/human activity prediction/test.csv")
test_orig.head()

train_orig=pd.read_csv("/content/drive/My Drive/human activity prediction/train.csv")
train_orig.head()

train_orig.Activity.value_counts()

train_orig.shape, test_orig.shape

test_orig.Activity.value_counts()

train_orig.columns

train_orig.describe()

from sklearn.utils import shuffle
train =shuffle(train_orig)
test= shuffle (test_orig)

traindata= train.drop('Activity', axis=1).values
trainlabel= train.Activity.values

testdata = test.drop('Activity', axis=1).values
testlabel = test.Activity.values

from sklearn import preprocessing

encoder= preprocessing.LabelEncoder()
encoder.fit(testLabel)
testlabelEN= encoder.transform(testlabel)

encoder.fit(trainlabel)
trainlabelEN= encoder.transform(trainlabel)

#mpl
import sklearn.neural_network as nn
mlpsgd = nn.MLPClassifier(
    hidden_layer_sizes=(90,),
    max_iter=1000, alpha= 1e-4,
    solver='sgd', verbose=10,
    tol=1e-19, random_state=1,
    learning_rate_init=0.001
)

#adam
mlpadam = nn.MLPClassifier(
    hidden_layer_sizes=(90,),
    max_iter=1000, alpha= 1e-4,
    solver='adam', verbose=10,
    tol=1e-19, random_state=1,
    learning_rate_init=0.001
)

#LBFGS
mlpbfgs = nn.MLPClassifier(
    hidden_layer_sizes=(90,),
    max_iter=1000, alpha= 1e-4,
    solver='lbfgs', verbose=10,
    tol=1e-19, random_state=1,
    learning_rate_init=0.001
)

nnmodelsdg = mlpsgd.fit(traindata, trainlabelEN)

nnmodelsdg

nnmodeladam= mlpadam.fit(traindata,trainlabelEN)

nnmodeladam

def get_all_data():
  train_values=train_orig.values
  test_values= test_orig.values

  np.random.shuffle(train_values)
  np.random.shuffle(test_values)

  X_train=train_values[:, :-1]
  Y_train=train_values[:, -1]
  X_test = test_values[:, :-1]
  Y_test = test_values[:, -1]
  
  return X_train,Y_train, X_test, Y_test

from sklearn.linear_model import LogisticRegression
X_train,Y_train, X_test, Y_test = get_all_data()

#Logistic Regression
model =LogisticRegression()
model

model.fit(X_train,Y_train)

#LR Accuracy
print("Logistic Regression Accuracy:")
model.score(X_test,Y_test)

#PCA
from sklearn.decomposition import PCA

X_train,  y_train, X_test, y_test = get_all_data()
pca= PCA(n_components=200)
pca.fit(X_train)

X_train = pca.transform(X_train)
X_test = pca.transform(X_test)

model.fit(X_train, y_train)
model.score(X_test, y_test)

#feature scaling
from sklearn.preprocessing import StandardScaler
scaler =StandardScaler()
X_train,Y_train, X_test, Y_test = get_all_data()
scaler.fit(X_train)

X_train =scaler.transform(X_train)
X_test=scaler.transform(X_test)

model.fit(X_train, Y_train)
model.score(X_test, Y_test)

#Random forest classifier
from sklearn.ensemble import RandomForestClassifier
X_train,Y_train, X_test, Y_test = get_all_data()
scaler.fit(X_train)

X_train =scaler.transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier()
model.fit(X_train, Y_train)
print("Random forest classifier accuracy : ")
model.score(X_test, Y_test)

#K-nearest neighbours
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

traindata= train.drop('Activity', axis=1).values
trainlabel = train.Activity.values

testdata = test.drop('Activity', axis=1).values
testlabel = test.Activity.values

encoder = preprocessing.LabelEncoder()

encoder.fit(testlabel)
testlabelEN=encoder.transform(testlabel)

encoder.fit(trainLabel)
trainlabelEN =encoder.transform(trainlabel)

clf=KNeighborsClassifier(n_neighbors=24)
knnmodel = clf.fit(traindata, trainlabelEN)
pred =clf.predict(testdata)

acc=accuracy_score(testlabelEN,pred)
print("KNN accuracy : %.5f" % (acc))

#decision tree classifier
from sklearn.tree import  DecisionTreeClassifier

dctreeclf= DecisionTreeClassifier()
tree =dctreeclf.fit(traindata, trainlabelEN)
testpred = tree.predict(testdata)
acc1= accuracy_score(testLabelE,testpred)
print('decision tree classifier Accuracy : %f'% acc1)

# Grid Search CV
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

parameters = {
    'kernel': ['linear', 'rbf', 'poly','sigmoid'],
    'C': [100, 50, 20, 1, 0.1]
}

selector = GridSearchCV(SVC(), parameters, scoring='accuracy') # we only care about accuracy here
selector.fit(trainData, trainLabel)

print('Best parameter set found:')
print(selector.best_params_)
print('Detailed grid scores:')
means = selector.cv_results_['mean_test_score']
stds = selector.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, selector.cv_results_['params']):
    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))
    print()

clf=SVC(kernel='linear', C=100).fit(trainData, trainLabel)
y_pred= clf.predict(testData)
print('Accuracy score:',accuracy_score(testLabel,y_pred))